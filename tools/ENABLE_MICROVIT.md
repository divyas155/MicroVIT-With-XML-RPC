# Enable MicroViT for Full AI Messages

## Issue
You're seeing simple fallback messages instead of full AI-generated natural language messages.

**Current output:**
```
Robot jetson1 at (20972.0, 1099235.3): LiDAR (REAL RPLIDAR) shows 0.245m to nearest obstacle.
```

**Expected output (with MicroViT enabled):**
```
âœ… Generated AI message with MicroViT+Ollama (FULL):
[Full natural language message describing what the robot sees, 
its situation, and suggested actions - generated by AI]
```

## Solution: Enable MicroViT

### Step 1: Create .env file on Orin

**SSH to Orin:**
```bash
ssh jetbot@10.13.68.159
cd ~/MicroVIT/robot1/orin_ros2_compute
```

**Create .env file:**
```bash
cat > .env << 'EOF'
# Robot Configuration
ROBOT_ID=jetson1

# Nano Connection
NANO_IP=10.13.68.184
NANO_PORT=8000

# Ollama Configuration
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=qwen2.5:0.5b
OLLAMA_NO_GPU=1

# MicroViT Configuration - ENABLE THIS!
USE_MICROVIT=true
MICROVIT_MODEL_NAME=apple/mobilevit-small
MICROVIT_VARIANT=S1
MICROVIT_USE_CPU=false

# Ollama Text Generation
OLLAMA_TEXT_MODEL=qwen2.5:0.5b
OLLAMA_MAX_TOKENS=500

# MQTT Configuration
MQTT_BROKER_HOST=localhost
MQTT_BROKER_PORT=1883
EOF
```

### Step 2: Restart AI Service with Environment Variables

**Stop current AI service (Ctrl+C), then restart:**

```bash
cd ~/MicroVIT/robot1/orin_ros2_compute
source venv/bin/activate

# Load environment variables from .env
export $(cat .env | grep -v '^#' | xargs)

# Or set manually:
export USE_MICROVIT=true
export NANO_IP=10.13.68.184
export NANO_PORT=8000
export OLLAMA_NO_GPU=1

cd src
python3 robot1_ai_service_realtime.py --simulation
```

### Step 3: Verify MicroViT is Enabled

**Look for these messages at startup:**
```
ðŸš€ Using MicroViT model for fast image preprocessing
âœ… MicroViT model loaded successfully!
âœ… Will use Ollama model 'qwen2.5:0.5b' for text generation
âš¡ MicroViT provides ~9ms preprocessing (vs ~500ms for BLIP)
```

**Then you should see full AI messages:**
```
âœ… Generated AI message with MicroViT+Ollama (FULL):
[Full natural language message appears here]
```

## Quick Fix (One Command)

**On Orin, restart with MicroViT enabled:**

```bash
cd ~/MicroVIT/robot1/orin_ros2_compute
source venv/bin/activate
export USE_MICROVIT=true
export NANO_IP=10.13.68.184
export NANO_PORT=8000
export OLLAMA_NO_GPU=1
cd src
python3 robot1_ai_service_realtime.py --simulation
```

## Troubleshooting

### If MicroViT fails to load:
- Check if `microvit_integration.py` exists in `common/utils/`
- Check if PyTorch is installed: `pip list | grep torch`
- Try CPU mode: `export MICROVIT_USE_CPU=true`

### If you see "âŒ Failed to load MicroViT":
- Check logs for specific error
- Verify model files are downloaded
- Try with CPU mode as fallback

## Expected Output After Enabling

You should see:
1. **At startup:**
   - `ðŸš€ Using MicroViT model for fast image preprocessing`
   - `âœ… MicroViT model loaded successfully!`

2. **During operation:**
   - `âœ… Generated AI message with MicroViT+Ollama (FULL):`
   - Followed by a full natural language message describing:
     - What the robot sees visually
     - LiDAR sensor readings
     - Combined situation assessment
     - Suggested actions

Instead of just:
- `Robot jetson1 at (x, y): LiDAR shows Xm to nearest obstacle.`
